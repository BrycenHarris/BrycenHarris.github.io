{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrycenHarris/BrycenHarris.github.io/blob/main/312_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_fwzU48S4Z6"
      },
      "source": [
        "[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giswqs/geog-312/blob/master/labs/lab_06.ipynb)\n",
        "[![image](https://binder.pangeo.io/badge_logo.svg)](https://gishub.org/geog312-pangeo)\n",
        "\n",
        "**Brycen Harris**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ZhM-pLS4Z9"
      },
      "source": [
        "## Question 1\n",
        "Data Manipulation (5 points): Write a Python function that takes a list of lists representing\n",
        "points (latitude, longitude) and returns the average latitude and average longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RaKV7OY_S4Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e75106-5aac-4bb3-f3bc-8e8e15ef68a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Latitude: 38.881033333333335\n",
            "Average Longitude: -93.29316666666666\n"
          ]
        }
      ],
      "source": [
        "def average_latitude_longitude(points):\n",
        "    total_points = len(points)\n",
        "\n",
        "    if total_points == 0:\n",
        "        return None  # Return None if the list is empty\n",
        "\n",
        "    total_latitude = sum(point[0] for point in points)\n",
        "    total_longitude = sum(point[1] for point in points)\n",
        "\n",
        "    avg_latitude = total_latitude / total_points\n",
        "    avg_longitude = total_longitude / total_points\n",
        "\n",
        "    return avg_latitude, avg_longitude\n",
        "\n",
        "# Example usage:\n",
        "points_list = [\n",
        "    [40.7128, -74.0060],\n",
        "    [34.0522, -118.2437],\n",
        "    [41.8781, -87.6298]\n",
        "]\n",
        "\n",
        "result = average_latitude_longitude(points_list)\n",
        "if result:\n",
        "    avg_lat, avg_long = result\n",
        "    print(f\"Average Latitude: {avg_lat}\")\n",
        "    print(f\"Average Longitude: {avg_long}\")\n",
        "else:\n",
        "    print(\"No points provided.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQgILLIcS4Z-"
      },
      "source": [
        "## Question 2\n",
        "Conditional Statements (5 points): Write a Python program that reads a CSV file containing\n",
        "elevation data for points. The program should then identify and print the points with an elevation\n",
        "above a user-specified threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NtuOEhwyS4Z-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "df593481-283a-4cfd-f458-fbc14d293bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the elevation threshold: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e808359a6679>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'elevation_data.csv'\u001b[0m  \u001b[0;31m# Replace with your CSV file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0muser_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the elevation threshold: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpoints_above_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_points_above_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ''"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def find_points_above_threshold(csv_file, threshold):\n",
        "    with open(csv_file, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip the header if present\n",
        "\n",
        "        above_threshold_points = []\n",
        "        for row in reader:\n",
        "            try:\n",
        "                latitude, longitude, elevation = map(float, row)\n",
        "                if elevation > threshold:\n",
        "                    above_threshold_points.append((latitude, longitude, elevation))\n",
        "            except ValueError:\n",
        "                print(\"Error: Skipping invalid row\")\n",
        "\n",
        "    return above_threshold_points\n",
        "\n",
        "# Example usage:\n",
        "csv_file_path = 'elevation_data.csv'  # Replace with your CSV file path\n",
        "user_threshold = float(input(\"Enter the elevation threshold: \"))\n",
        "\n",
        "points_above_threshold = find_points_above_threshold(csv_file_path, user_threshold)\n",
        "\n",
        "if points_above_threshold:\n",
        "    print(f\"Points with elevation above {user_threshold}:\")\n",
        "    for point in points_above_threshold:\n",
        "        print(f\"Latitude: {point[0]}, Longitude: {point[1]}, Elevation: {point[2]}\")\n",
        "else:\n",
        "    print(\"No points found above the specified threshold.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5XLUP0QS4Z-"
      },
      "source": [
        "## Question 3\n",
        "Loops (5 points): Write a Python program that iterates through a dictionary containing county\n",
        "names and their corresponding population densities. The program should calculate and print the\n",
        "total population for all counties.\n",
        "(Hint: Use a random number generator or google the population of your chosen cities to create\n",
        "the values for the population density. Use 10 different city names.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "exaM4L9OS4Z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60aaef3-2b5a-4555-f97b-569c35874e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total population for all counties is: 35,945,000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Sample dictionary containing county names and their population densities\n",
        "county_population_densities = {\n",
        "    'Los Angeles County': 2410,\n",
        "    'Cook County': 5495,\n",
        "    'Harris County': 2825,\n",
        "    'Maricopa County': 1286,\n",
        "    'San Diego County': 815,\n",
        "    'Orange County': 4016,\n",
        "    'Miami-Dade County': 1872,\n",
        "    'Dallas County': 2604,\n",
        "    'Kings County': 14237,\n",
        "    'Riverside County': 385\n",
        "}\n",
        "\n",
        "# Function to calculate total population\n",
        "def calculate_total_population(county_population):\n",
        "    total_population = 0\n",
        "    for county, density in county_population.items():\n",
        "        # Assume an average county area of 1000 square miles for population calculation\n",
        "        population = density * 1000\n",
        "        total_population += population\n",
        "    return total_population\n",
        "\n",
        "# Calculate and print the total population\n",
        "total_population = calculate_total_population(county_population_densities)\n",
        "print(f\"The total population for all counties is: {total_population:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa9DEA-MS4Z-"
      },
      "source": [
        "## Question 4\n",
        " Functions with Arguments (5 points): Write a Python function that takes a shapefile path as\n",
        "input and returns its area in square kilometers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dVHvCmevS4Z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e3c4f9-3aef-4d92-9d86-58785568a8ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:fiona._env:path/to/your/shapefile.shp: No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: path/to/your/shapefile.shp: No such file or directory\n",
            "Failed to calculate the area.\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "def calculate_shapefile_area(shapefile_path):\n",
        "    try:\n",
        "        # Read the shapefile using geopandas\n",
        "        gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "        # Calculate the total area of the shapefile\n",
        "        total_area_sq_m = gdf.geometry.area.sum()\n",
        "\n",
        "        # Convert square meters to square kilometers\n",
        "        area_sq_km = total_area_sq_m / 1e6  # 1 square kilometer = 1,000,000 square meters\n",
        "\n",
        "        return area_sq_km\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "shapefile_path = 'path/to/your/shapefile.shp'  # Replace with the path to your shapefile\n",
        "area = calculate_shapefile_area(shapefile_path)\n",
        "\n",
        "if area is not None:\n",
        "    print(f\"The area of the shapefile is: {area:.2f} square kilometers\")\n",
        "else:\n",
        "    print(\"Failed to calculate the area.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKXkVHiVS4Z_"
      },
      "source": [
        "## Question 5\n",
        "Error Handling (5 points): Write a Python program that attempts to read a raster file. If the file\n",
        "is not found or invalid, the program should print a helpful error message and gracefully exit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2X8ICW_YS4Z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebf6ea6-026f-4841-eae6-5eebe9b1e9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File 'path/to/your/raster/file.tif' not found or invalid.\n"
          ]
        }
      ],
      "source": [
        "from osgeo import gdal\n",
        "\n",
        "def read_raster_file(file_path):\n",
        "    try:\n",
        "        # Attempt to open the raster file\n",
        "        raster_dataset = gdal.Open(file_path)\n",
        "\n",
        "        if raster_dataset is None:\n",
        "            raise FileNotFoundError(f\"File '{file_path}' not found or invalid.\")\n",
        "\n",
        "        print(\"Raster file read successfully.\")\n",
        "        # Close the dataset when done\n",
        "        raster_dataset = None\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        # Gracefully exit the program if file not found or invalid\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        # Handle other potential exceptions\n",
        "\n",
        "# Example usage:\n",
        "raster_file_path = 'path/to/your/raster/file.tif'  # Replace with the path to your raster file\n",
        "read_raster_file(raster_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2**"
      ],
      "metadata": {
        "id": "OwTAR_SKTxHu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsd6IFk4S4Z_"
      },
      "source": [
        "## Question 1\n",
        "Problem-Solving (10 points): You are tasked with creating a program to identify areas suitable\n",
        "for building a new solar farm. You have access to datasets for land cover, slope, and solar\n",
        "radiation. Describe the workflow of your program, including data preparation, analysis steps, and\n",
        "final output."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation:\n",
        "a. Data Acquisition: Acquire datasets for land cover, slope, and solar radiation. These datasets can also be obtained from various sources like satellite imagery, government databases, or remote sensing data providers.\n",
        "\n",
        "b. Data Preprocessing:\n",
        "- Ensure the datasets are in compatible formats (raster or vector).\n",
        "- Check for any missing or invalid data. Handle or clean the data as necessary.\n",
        "- Reproject or align datasets to a common coordinate system for consistency.\n",
        "\n",
        "2. Analysis Steps:\n",
        "a. Combine Datasets: Overlay or merge the land cover, slope, and solar radiation datasets spatially to work with integrated information.\n",
        "\n",
        "b. Suitability Criteria Definition: Define criteria for suitable areas for a solar farm. This might include:\n",
        "- Flat or low slope areas (as steep slopes are generally not ideal).\n",
        "- Areas with land cover types suitable for solar farms (e.g., barren land, open space, etc.).\n",
        "- High solar radiation zones for optimal energy generation.\n",
        "\n",
        "c. Assign Weightings and Ranks: Assign weights or ranks to different criteria based on their importance. For instance, flat areas might be more critical than specific land cover types.\n",
        "\n",
        "d. Overlay and Analysis: Use GIS (Geographic Information System) tools or libraries (like geopandas, rasterio, gdal, etc.) in Python to overlay and analyze the datasets based on the defined criteria. Combine the factors using weighted or ranked overlay analysis to generate a suitability map.\n",
        "\n",
        "3. Final Output:\n",
        "a. Suitability Map: Generate a suitability map highlighting areas suitable for building a solar farm based on the analysis. This map can indicate areas ranked or weighted by their suitability for the project.\n",
        "\n",
        "b. Visualization: Visualize the suitability map using plotting libraries like GIS software to display areas suitable for the solar farm.\n",
        "\n",
        "c. Summary Report: Create a summary report detailing the methodology, input data, analysis steps, and the final recommended areas for building the solar farm."
      ],
      "metadata": {
        "id": "wKvhgROGXzAa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shQUf7VvS4Z_"
      },
      "source": [
        "## Question 2\n",
        "Critical Thinking (10 points): You are given a shapefile of earthquake epicenters and a raster\n",
        "file of population density. Explain how you would use Python and GIS libraries to identify the\n",
        "areas most vulnerable to earthquake damage, considering both population density and proximity\n",
        "to epicenters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation:\n",
        "Load the Shapefile and Raster File: Use geopandas or similar libraries to load the earthquake epicenters shapefile and the population density raster file.\n",
        "2. Analysis Steps:\n",
        "a. Proximity to Epicenters:\n",
        "- Calculate the distance or buffer around each earthquake epicenter to create zones of influence using spatial analysis tools.\n",
        "- Assign weights or create distance bands to represent the level of vulnerability based on proximity to epicenters (e.g., higher vulnerability closer to epicenters).\n",
        "\n",
        "b. Population Density:\n",
        "- Extract population density values from the raster file corresponding to each location in the earthquake epicenters shapefile. This can be done using GIS operations or libraries like rasterio.\n",
        "- Normalize population density values if necessary to standardize the comparison across different areas.\n",
        "\n",
        "c. Combine Factors:\n",
        "- Overlay the earthquake epicenters buffer zones (from step a) and the population density data.\n",
        "- Assign weights to the layers based on their importance in vulnerability assessment (e.g., higher weight for high population density and close proximity to epicenters).\n",
        "\n",
        "d. Vulnerability Index Calculation:\n",
        "- Combine the weighted factors (proximity to epicenters and population density) using an appropriate method such as a weighted overlay analysis or simple addition/multiplication, considering the assigned weights.\n",
        "- Generate a vulnerability index or vulnerability map representing the combined impact of both factors.\n",
        "\n",
        "3. Result Interpretation:\n",
        "Identify Vulnerable Areas: Areas with higher vulnerability index values will indicate higher susceptibility to earthquake damage.\n",
        "\n",
        "Visualization: Generate visual representations (maps, charts) using plotting libraries like matplotlib or GIS software to visualize the vulnerability index or vulnerability map.\n",
        "\n",
        "4. Further Analysis (Optional):\n",
        "Validation: Validate the vulnerability assessment by comparing against historical earthquake damage records or other ground-truth data.\n",
        "\n",
        "Sensitivity Analysis: Conduct sensitivity analysis by altering weightings or criteria to understand their impact on vulnerability assessment."
      ],
      "metadata": {
        "id": "i2AKUn0nYexV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDskaXUSS4aA"
      },
      "source": [
        "## Question 3\n",
        "Critical Thinking (10 points): You are tasked with creating a visualization that shows the\n",
        "impacts of climate change on Tennessee agricultural production. You are given a dataset for\n",
        "precipitation, temperature, and crop output. How would you go about visualizing this in python\n",
        "and what analyses would need to be done to measure these impacts?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation:\n",
        "a. Data Acquisition: Obtain datasets for precipitation, temperature, and crop output in Tennessee over a significant time period. This data can also be obtained from sources like government agencies (NOAA, USDA), research institutions, or datasets available online.\n",
        "\n",
        "b. Data Cleaning and Integration: Clean, preprocess, and integrate the datasets. Ensure they cover the same time span and spatial resolution. Convert them into a structured format suitable for analysis using Python.\n",
        "\n",
        "2. Analysis Steps:\n",
        "a. Temporal Analysis:\n",
        "- Analyze the trends in temperature and precipitation over time using statistical methods (e.g., moving averages, trend analysis) to identify patterns of change.\n",
        "\n",
        "b. Correlation Analysis:\n",
        "- Examine the correlation between temperature, precipitation, and crop output. Determine how changes in climate variables affect crop yields using statistical correlation measures (e.g., Pearson's correlation coefficient).\n",
        "\n",
        "c. Crop Yield Analysis:\n",
        "- Analyze historical crop output data to identify trends or variations in agricultural production over time.\n",
        "\n",
        "d. Comparison and Anomaly Detection:\n",
        "- Compare the trends in temperature, precipitation, and crop output to detect anomalies or deviations from historical averages or expected patterns.\n",
        "\n",
        "3. Visualization:\n",
        "a. Time Series Plots: Create time series plots to visualize changes in temperature, precipitation, and crop yield over the years.\n",
        "\n",
        "b. Correlation Heatmap: Generate a heatmap or correlation matrix to visualize the relationships between temperature, precipitation, and crop output. This will demonstrate how changes in climate variables impact agricultural production.\n",
        "\n",
        "c. Anomaly Detection Plots: Plot anomalies or deviations from the expected values for temperature, precipitation, and crop output to highlight abnormal patterns that could indicate climate change impacts.\n",
        "\n",
        "d. Geospatial Visualization (Optional): Use geospatial visualization techniques to map the spatial distribution of temperature, precipitation, or crop output changes across different regions in Tennessee.\n",
        "\n",
        "4. Interpretation and Communication:\n",
        "a. Interpret Findings: Analyze the visualizations and findings to understand the impacts of climate change on agricultural production in Tennessee.\n",
        "\n",
        "b. Communicate Results: Create a comprehensive report or presentation summarizing the analyses, visualizations, and conclusions drawn from the data. Clearly communicate the observed trends, correlations, anomalies, and their implications for agriculture due to climate change."
      ],
      "metadata": {
        "id": "va3gQY3mZDRk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tFRFKF4S4aA"
      },
      "source": [
        "## Question 4\n",
        "Geospatial Analysis (10 points): You have a shapefile of national parks and a raster file of\n",
        "deforestation rates. Design a Python program to calculate the total area of deforestation within\n",
        "each national park and identify the park with the highest deforestation rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation:\n",
        "a. Load Shapefile and Raster File: Use geopandas to load the shapefile of national parks and rasterio to load the raster file of deforestation rates.\n",
        "\n",
        "2. Geospatial Analysis:\n",
        "a. Overlay Analysis:\n",
        "- For each national park polygon, overlay it with the deforestation raster data to extract the deforestation rate within each park boundary.\n",
        "- Calculate the area of deforestation within each park by multiplying the deforestation rate by the park's area.\n",
        "\n",
        "b. Calculate Total Deforestation Area:\n",
        "- Sum up the total area of deforestation within each national park to get the overall deforested area for each park.\n",
        "\n",
        "c. Identify Park with Highest Deforestation Rate:\n",
        "- Determine the park with the highest total deforestation area or the highest deforestation rate relative to its area.\n",
        "\n",
        "3. Result Output:\n",
        "Print the total area of deforestation within each national park.\n",
        "Identify and print the park with the highest deforestation rate or total deforested area."
      ],
      "metadata": {
        "id": "dHXbgV3MZgsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG3HSXJSS4aA"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "\n",
        "# Load national parks shapefile\n",
        "national_parks = gpd.read_file('path/to/national_parks.shp')\n",
        "\n",
        "# Load deforestation rates raster file\n",
        "deforestation_raster = rasterio.open('path/to/deforestation_rates.tif')\n",
        "\n",
        "# Initialize variables to track park with highest deforestation\n",
        "highest_deforestation_area = 0\n",
        "park_with_highest_deforestation = None\n",
        "\n",
        "# Loop through each national park\n",
        "for index, park in national_parks.iterrows():\n",
        "    # Clip deforestation raster with park boundary\n",
        "    park_deforestation, _ = mask(deforestation_raster, [park.geometry], crop=True)\n",
        "\n",
        "    # Calculate area of deforestation within the park\n",
        "    deforestation_area = park_deforestation.sum()  # Modify this based on raster units\n",
        "\n",
        "    # Update total deforestation area for the park\n",
        "    national_parks.at[index, 'Deforestation_Area'] = deforestation_area\n",
        "\n",
        "    # Check for park with highest deforestation\n",
        "    if deforestation_area > highest_deforestation_area:\n",
        "        highest_deforestation_area = deforestation_area\n",
        "        park_with_highest_deforestation = park['Park_Name']\n",
        "\n",
        "# Calculate total deforestation area for each park\n",
        "total_deforestation_by_park = national_parks.groupby('Park_Name')['Deforestation_Area'].sum()\n",
        "\n",
        "# Print total deforestation area for each park\n",
        "print(\"Total deforestation area within each national park:\")\n",
        "print(total_deforestation_by_park)\n",
        "\n",
        "# Print park with highest deforestation rate or total deforested area\n",
        "print(f\"\\nThe park with the highest deforestation rate is: {park_with_highest_deforestation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3**"
      ],
      "metadata": {
        "id": "DiglGPsLUO4M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Rp2gi_S4aA"
      },
      "source": [
        "## Question 1\n",
        "Shapefile Visualization (10 points): Write a Python program to read a shapefile. Create a map\n",
        "showing the data points as colored polygons (anything other than circles). Each data point should\n",
        "be colored based on its magnitude (e.g., green for small (low), yellow for medium (average), red\n",
        "for large (high)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdbTD3pBS4aA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCagV4HS4aA"
      },
      "source": [
        "## Question 2\n",
        "Raster Processing and Visualization (10 points): Write a Python program to read a raster file of\n",
        "precipitation data and apply a colormap to visualize the temperature variations. Add a legend to\n",
        "the map and ensure clear labels for axes and title.\n",
        "*Hint: Use the precipitation dataset already provided to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or6C1IZNS4aA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGdN_rssS4aA"
      },
      "source": [
        "## Question 3\n",
        "Cartographic Design (10 points): Design a map showing the distribution of solar farms in\n",
        "west, middle, and east Tennessee. Create a raster layer that summarizes the total energy\n",
        "generated from the solar farms in each of these regions (it can be one raster layer). Discuss which\n",
        "region has the most solar production and why.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7MF0JSTS4aA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8inivL1S4aA"
      },
      "source": [
        "## Bonus Question (10 points)\n",
        "Develop a Python program that automates a repetitive GIS task you encountered during your\n",
        "project or coursework. Explain the task, your approach, and the benefits of automation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So for my project I dont believe I had an task that I would say were repetative seeing how I used a different tool everytime but I will use Delete Feature as an example because I did use it multiple times but for a different dataset each time. I do not remeber the path I used for the code so I will just put in a place holder where neeeded.\n",
        "\n",
        "This is my best guess as a code to use for automation"
      ],
      "metadata": {
        "id": "z9XCs38ae62b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhB8cwlpS4aB"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# Load the shapefile containing cities' spatial data\n",
        "cities_data = gpd.read_file('path/to/cities.shp')\n",
        "\n",
        "# Load the boundary of Knoxville, Tennessee (assuming it's also a shapefile or GeoJSON)\n",
        "knoxville_boundary = gpd.read_file('path/to/knoxville_boundary.shp')\n",
        "\n",
        "# Perform spatial operation to select cities within Knoxville boundary\n",
        "cities_within_knoxville = gpd.overlay(cities_data, knoxville_boundary, how='intersection')\n",
        "\n",
        "# Save the updated dataset containing cities within Knoxville boundary\n",
        "cities_within_knoxville.to_file('path/to/updated_cities.shp', driver='ESRI Shapefile')\n",
        "\n",
        "# Optional: If you want to delete the original file after creating the updated one\n",
        "import os\n",
        "os.remove('path/to/cities.shp')\n",
        "os.remove('path/to/cities.shx')\n",
        "os.remove('path/to/cities.dbf')\n",
        "os.remove('path/to/cities.prj')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}